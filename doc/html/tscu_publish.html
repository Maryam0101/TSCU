
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
<script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-51925957-1', 'timewarping.org'); ga('require', 'displayfeatures'); ga('send', 'pageview'); </script>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>tscu_publish</title><meta name="generator" content="MATLAB 8.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2014-06-19"><meta name="DC.source" content="tscu_publish.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">Using Support Vector Machines (SVM) classification</a></li><li><a href="#4">Comparing SVM with KNN under different alignments</a></li><li><a href="#7">Tuning soft margin parameter of SVM</a></li><li><a href="#11">Using nonlinear kernel for SVM</a></li><li><a href="#13">Tuning gaussian kernel</a></li></ul></div><p>Publish tscu_tutorial.m in PDF and HTML format for displaying on the web site.</p><pre class="codeinput"><span class="comment">%publish('tscu_tutorial.m','format','pdf' ,'outputDir','../doc/pdf');</span>
<span class="comment">%publish('tscu_tutorial.m','format','html','outputDir','../doc/html');</span>
<span class="comment">%publish('tscu.m','format','html','outputDir','../doc/html');</span>
close <span class="string">all</span>
</pre><h2>Using Support Vector Machines (SVM) classification<a name="2"></a></h2><p>From the beginning, we have been using Nearest Neighbor classification scheme which is the default classifier in TSCU. However you have an alternative: Support Vector Machines (SVM). If you want to give it a try, you should use the option <tt>Classifier</tt>. The default kernel type of SVM is linear with C=10.</p><p>The implementation of SVM is based on the MATLAB&reg; scripts in the book "Support Vector Machines for Antenna Array Processing and Electromagnetics", Manel Martinez-Ramon, Christos G. Christodoulou, Morgan &amp; Claypool Publishers, 2006. ISSN <a href="http://www.amazon.com/dp/159829024X">159829024X</a></p><pre class="codeinput">tscu(trn,tst,<span class="string">'Classifier'</span>,<span class="string">'SVM'</span>);
</pre><pre class="codeoutput">Size of training set.....................: 300
Size of testing set......................: 300
Time series length.......................: 60
Classification method....................: SVM
SVM kernel type..........................: linear
SVM Soft margin..........................:  8.00000
Alignment method.........................: NONE
Displaying input data....................: no
No cross validation is chosen............: 0
Displaying alignments....................: none
Dumping distance matrix..................: no
Class information........................: 1 [TRN: 50 TST: 50]
Class information........................: 2 [TRN: 50 TST: 50]
Class information........................: 3 [TRN: 50 TST: 50]
Class information........................: 4 [TRN: 50 TST: 50]
Class information........................: 5 [TRN: 50 TST: 50]
Class information........................: 6 [TRN: 50 TST: 50]
Overall Accuracy.........................: 0.927   
Overall Error............................: 0.073   
Producer Accuracy........................: 0.800   1.000   0.920   0.920   0.960   0.960   
User Accuracy............................: 1.000   0.833   0.958   0.958   0.923   0.923   
Kappa....................................: 0.912   
Z-value..................................: 7.152   
Confusion matrix.........................: 
Confusion matrix
          1     2     3     4     5     6    UA    TO 
    1    40     0     0     0     0     0 1.000    40 
    2    10    50     0     0     0     0 0.833    60 
    3     0     0    46     0     2     0 0.958    48 
    4     0     0     0    46     0     2 0.958    48 
    5     0     0     4     0    48     0 0.923    52 
    6     0     0     0     4     0    48 0.923    52 
   PA 0.800 1.000 0.920 0.920 0.960 0.960 
   TO    50    50    50    50    50    50         300 

Classification time (sec)................: 0.45    
The end of TSCU..........................: FINISHED
</pre><p>Let's analyze the results. First of all it took around 1 seconds to do all the classification whereas KNN required approximately 10 seconds. So it is clearly faster than KNN. But remember the exact running times will change depending on the number and speed of the processors you have. Don't be surprized if you have very different running times. If you have a fast CPU, both SVM and KNN may run under a second. If that is the case you may not notice a difference between the two. On the other hand, if you have a slow PC, then the difference between the two will be dramatic.</p><p>The next thing to look at is the accuracy. The overall accuracy of SVM 92.7% which is higher than KNN (remember that, it was 88%). If you compare the confusion matrices, you will see that SVM is better from KNN at  discriminating the first class from other classes. However it is still having a hard time to separete the first class from the second as it  makes 10 misclassifications (see the 2nd row and first column entry of the confusion matrix). Other than that SVM is make only 4 misclassifications.</p><h2>Comparing SVM with KNN under different alignments<a name="4"></a></h2><p>We compared SVM with KNN without using alignment or technically speaking alignment with NONE. In this case, SVM seems better than KNN in terms of both speed and accuracy. But what about if I use other alignments such as DTW and CDTW? I will run TSCU with different options and prepare a table similar to the below.</p><pre>                         Alignment
                    -------------------
Classification      NONE    DTW    CDTW
KNN                 ...     ...    ...
SVM                 ...     ...    ...</pre><pre class="codeinput">svm_none=tscu(trn,tst,<span class="string">'Classifier'</span>,<span class="string">'SVM'</span>,<span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>);
svm_dtw =tscu(trn,tst,<span class="string">'Classifier'</span>,<span class="string">'SVM'</span>,<span class="string">'Alignment'</span>,<span class="string">'DTW'</span>,<span class="keyword">...</span>
          <span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>);
svm_cdtw=tscu(trn,tst,<span class="string">'Classifier'</span>,<span class="string">'SVM'</span>,<span class="string">'Alignment'</span>,<span class="string">'CDTW'</span>,<span class="keyword">...</span>
          <span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>);
knn_none=tscu(trn,tst,<span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>);
knn_dtw =tscu(trn,tst,<span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>,<span class="string">'Alignment'</span>,<span class="string">'DTW'</span>);
knn_cdtw=tscu(trn,tst,<span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>,<span class="string">'Alignment'</span>,<span class="string">'CDTW'</span>);
</pre><p>I have the results. Now, in order to create the table I mentioned above, I have to use these ugly looking commands. Sorry! You don't have to use these commands but I prefer to create these kind of text tables.</p><pre class="codeinput">fprintf(<span class="string">'%12s %-17s\n'</span>,<span class="string">''</span>,<span class="string">'Alignment'</span>);
fprintf(<span class="string">'%12s %s\n'</span>,<span class="string">''</span>,<span class="string">'-----------------'</span>);
fprintf(<span class="string">'%12s %-5s %-5s %-5s\n'</span>,<span class="string">'Classifier'</span>,<span class="string">'NONE'</span>,<span class="string">'DTW'</span>,<span class="string">'CDTW'</span>);
fprintf(<span class="string">'%12s %3.1f%% %3.1f%% %3.1f%%\n'</span>,<span class="string">'KNN'</span>,100*knn_none.perf.OA,<span class="keyword">...</span>
    100*knn_dtw.perf.OA,100*knn_cdtw.perf.OA);
fprintf(<span class="string">'%12s %3.1f%% %3.1f%% %3.1f%%\n'</span>,<span class="string">'SVM'</span>,100*svm_none.perf.OA,<span class="keyword">...</span>
    100*svm_dtw.perf.OA,100*svm_cdtw.perf.OA);
</pre><pre class="codeoutput">             Alignment        
             -----------------
  Classifier NONE  DTW   CDTW 
         KNN 88.0% 99.3% 98.7%
         SVM 92.7% 91.7% 91.7%
</pre><p>As you see, if I don't use alignment, then SVM is definitely better than KNN. But using alignment does not improve the accuracy of SVM.  Contrary it slightly decreases the accuracy. So, I can claim that the classification accuracy depends not only on the classification method, but also on the alignment method. The two are tighly coupled. A third dependancy is data. The performance of classification algorithms heavily depends on the type of data. In the above example, KNN with NONE alignment is better than SVM with NONE alignment. But you find a dataset for which the opposite is true. In summary, the classification accuracy depends on three main factors:</p><div><ul><li>classification method (KNN,SVM)</li><li>alignment method (NONE,DTW,CDTW)</li><li>data (Synthetic Control,...)</li></ul></div><h2>Tuning soft margin parameter of SVM<a name="7"></a></h2><p>Like many other machine learning algorithms, SVM has also some parameters that controls the details of the algorithm. The most well known parameter is the soft margin parameter; usually denoted by C. It is very useful for the cases where it is impossible to obtain a separating hyperlane without any misclassification. Setting C to a non-zero value, you basically give some flexibility to SVM. In doing so, SVM is allowed to make some errors. Increasing the soft margin parameter give higher flexibility. For further information, you can take a look at Cortes, C.;Vapnik, V. (1995), "Support-vector networks". Machine Learning 20 (3): 273 <a href="http://dx.doi.org/10.1007%2FBF00994018">doi:10.1007/BF00994018</a></p><p>The default soft margin parameter is 8 in TSCU. You can change it by using <tt>SVMSoftMargin</tt> option. Here I will try 11 different parameters and plot the resulting classification accuracies. It is common to try powers of 2, so I will follow the general trend. The parameters that I used are C={pow(2,i) where i=-5,...,5}</p><pre class="codeinput">margins=2.^(-5:5);
accuracies=zeros(1,numel(margins));
<span class="keyword">for</span> i=1:length(margins)
    out=tscu(trn,tst,<span class="string">'Classifier'</span>,<span class="string">'SVM'</span>,<span class="string">'SVMSoftMargin'</span>,margins(i),<span class="keyword">...</span>
        <span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>);
    accuracies(i)=out.perf.OA;
<span class="keyword">end</span>
</pre><p>Then I will run tscu in silent mode (remember the <tt>LogLevel</tt> option), collect the accuracies and plot them.</p><pre class="codeinput">figure
plot(log2(margins),100*accuracies,<span class="string">'o-'</span>);
xlabel(<span class="string">'Soft margin parameter (log2)'</span>);
ylabel(<span class="string">'Overall accuracy (%)'</span>);
</pre><p>Here are the classificaton accuracies over different soft margins. As you see, the results didn't change except C=1/8 and C=1/4. But the change in the accuracy is around 0.006 which is very small and negligible. Therefore, changing the soft margin parameter of SVM didn't improve the accuracy at least in this dataset and alignment scheme.</p><img vspace="5" hspace="5" src="tscu_publish_01.png" alt=""> <h2>Using nonlinear kernel for SVM<a name="11"></a></h2><p>TSCU allows you to change the default linear kernel used in SVM to a nonlinear gaussian kernel. For this, you have assign <tt>SVMKernel</tt> option to <tt>gaussian</tt>.</p><pre class="codeinput">tscu(trn,tst,<span class="string">'Classifier'</span>,<span class="string">'SVM'</span>,<span class="string">'SVMKernel'</span>,<span class="string">'gaussian'</span>);
</pre><pre class="codeoutput">Size of training set.....................: 300
Size of testing set......................: 300
Time series length.......................: 60
Classification method....................: SVM
SVM kernel type..........................: gaussian
SVM Soft margin..........................:  8.00000
SVM sigma parameter......................:  1.00000
Alignment method.........................: NONE
Displaying input data....................: no
No cross validation is chosen............: 0
Displaying alignments....................: none
Dumping distance matrix..................: no
Class information........................: 1 [TRN: 50 TST: 50]
Class information........................: 2 [TRN: 50 TST: 50]
Class information........................: 3 [TRN: 50 TST: 50]
Class information........................: 4 [TRN: 50 TST: 50]
Class information........................: 5 [TRN: 50 TST: 50]
Class information........................: 6 [TRN: 50 TST: 50]
Overall Accuracy.........................: 0.523   
Overall Error............................: 0.477   
Producer Accuracy........................: 1.000   0.460   0.340   0.440   0.420   0.480   
User Accuracy............................: 0.259   1.000   1.000   1.000   1.000   1.000   
Kappa....................................: 0.428   
Z-value..................................: 1.918   
Confusion matrix.........................: 
Confusion matrix
          1     2     3     4     5     6    UA    TO 
    1    50    27    33    28    29    26 0.259   193 
    2     0    23     0     0     0     0 1.000    23 
    3     0     0    17     0     0     0 1.000    17 
    4     0     0     0    22     0     0 1.000    22 
    5     0     0     0     0    21     0 1.000    21 
    6     0     0     0     0     0    24 1.000    24 
   PA 1.000 0.460 0.340 0.440 0.420 0.480 
   TO    50    50    50    50    50    50         300 

Classification time (sec)................: 0.97    
The end of TSCU..........................: FINISHED
</pre><p>You can see the kernel type of SVM in the info messages. Other than that, the rest of the messages are same as before except the classification accuracy: 50% which is very low compared to the linear SVM. The confusion matrix tells us that the first class (normal) is confused with the other classes.</p><h2>Tuning gaussian kernel<a name="13"></a></h2><p>You can change the gaussian kernel parameter sigma by using the option <tt>SVMSigma</tt>. Here I will try the following sigma values: s={pow(2,i) where i=-5,...,5}. Then I will run tscu in silent mode (remember the <tt>LogLevel</tt> option) collect the accuracies and plot them.</p><pre class="codeinput">sigmas=2.^(-5:5);
accuracies=zeros(1,numel(sigmas));
<span class="keyword">for</span> i=1:length(sigmas)
    out=tscu(trn,tst,<span class="string">'Classifier'</span>,<span class="string">'SVM'</span>,<span class="string">'SVMKernel'</span>,<span class="string">'gaussian'</span>,<span class="keyword">...</span>
        <span class="string">'SVMSigma'</span>,sigmas(i),<span class="keyword">...</span>
        <span class="string">'LogLevel'</span>,<span class="string">'Alert'</span>);
    accuracies(i)=out.perf.OA;
<span class="keyword">end</span>
</pre><pre class="codeinput">figure
plot(log2(sigmas),100*accuracies,<span class="string">'o-'</span>);
xlabel(<span class="string">'sigma parameter of Gaussian kernel (log2)'</span>);
ylabel(<span class="string">'Overall accuracy (%)'</span>);
</pre><img vspace="5" hspace="5" src="tscu_publish_02.png" alt=""> <p>As you see, the classification accuracy heavily depends of the sigma parameter. As it is being increased, the overall accuracy increases. The largest accuracy (the y-axis of the biggest peak) is 97.67\%.  This is very high compared to the linear SVM.</p><pre class="codeinput">fprintf(<span class="string">'Maximum accuracy: %8.2f\n'</span>,100*max(accuracies));
</pre><pre class="codeoutput">Maximum accuracy:    97.67
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014a</a><br></p></div><!--
##### SOURCE BEGIN #####
%%
% Publish tscu_tutorial.m in PDF and HTML format for displaying on the web
% site.
%publish('tscu_tutorial.m','format','pdf' ,'outputDir','../doc/pdf');
%publish('tscu_tutorial.m','format','html','outputDir','../doc/html');
%publish('tscu.m','format','html','outputDir','../doc/html');
close all
%% Using Support Vector Machines (SVM) classification
% From the beginning, we have been using Nearest Neighbor classification
% scheme which is the default classifier in TSCU. However you have an
% alternative: Support Vector Machines (SVM). If you want to give it a try,
% you should use the option |Classifier|. The default kernel type of SVM is
% linear with C=10. 
% 
% The implementation of SVM is based on the MATLAB(R) scripts in the book 
% "Support Vector Machines for Antenna Array Processing and 
% Electromagnetics", Manel Martinez-Ramon, Christos G. Christodoulou, 
% Morgan & Claypool Publishers, 2006. 
% ISSN <http://www.amazon.com/dp/159829024X 159829024X>
tscu(trn,tst,'Classifier','SVM');
%%
% Let's analyze the results. First of all it took around 1 seconds to do 
% all the classification whereas KNN required approximately 10 seconds.
% So it is clearly faster than KNN. But remember the exact running times
% will change depending on the number and speed of the processors you have.
% Don't be surprized if you have very different running times. If you have
% a fast CPU, both SVM and KNN may run under a second. If that is the case
% you may not notice a difference between the two. On the other
% hand, if you have a slow PC, then the difference between the two will be
% dramatic.
%
% The next thing to look at is the accuracy. The overall accuracy of SVM
% 92.7% which is higher than KNN (remember that, it was 88%). If you 
% compare the confusion matrices, you will see that SVM is better from KNN 
% at  discriminating the first class from other classes. However it is 
% still having a hard time to separete the first class from the second as 
% it  makes 10 misclassifications (see the 2nd row and first column entry 
% of the confusion matrix). Other than that SVM is make only 4 
% misclassifications.
%% Comparing SVM with KNN under different alignments
% We compared SVM with KNN without using alignment or technically speaking
% alignment with NONE. In this case, SVM seems better than KNN in terms of
% both speed and accuracy. But what about if I use other alignments such as
% DTW and CDTW? I will run TSCU with different options and prepare a table
% similar to the below.
%
%                           Alignment
%                      REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
%  Classification      NONE    DTW    CDTW
%  KNN                 ...     ...    ...
%  SVM                 ...     ...    ...
svm_none=tscu(trn,tst,'Classifier','SVM','LogLevel','Alert');
svm_dtw =tscu(trn,tst,'Classifier','SVM','Alignment','DTW',...
          'LogLevel','Alert');
svm_cdtw=tscu(trn,tst,'Classifier','SVM','Alignment','CDTW',...
          'LogLevel','Alert');
knn_none=tscu(trn,tst,'LogLevel','Alert');
knn_dtw =tscu(trn,tst,'LogLevel','Alert','Alignment','DTW');
knn_cdtw=tscu(trn,tst,'LogLevel','Alert','Alignment','CDTW');
%%
% I have the results. Now, in order to create the table I mentioned above, 
% I have to use these ugly looking commands. Sorry! You don't have to use
% these commands but I prefer to create these kind of text tables. 
fprintf('%12s %-17s\n','','Alignment');
fprintf('%12s %s\n','','REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-');
fprintf('%12s %-5s %-5s %-5s\n','Classifier','NONE','DTW','CDTW');
fprintf('%12s %3.1f%% %3.1f%% %3.1f%%\n','KNN',100*knn_none.perf.OA,...
    100*knn_dtw.perf.OA,100*knn_cdtw.perf.OA);
fprintf('%12s %3.1f%% %3.1f%% %3.1f%%\n','SVM',100*svm_none.perf.OA,...
    100*svm_dtw.perf.OA,100*svm_cdtw.perf.OA);
%%
% As you see, if I don't use alignment, then SVM is definitely better than
% KNN. But using alignment does not improve the accuracy of SVM.  Contrary 
% it slightly decreases the accuracy. So, I can claim that the
% classification accuracy depends not only on the classification method, 
% but also on the alignment method. The two are tighly coupled. A third 
% dependancy is data. The performance of classification algorithms heavily
% depends on the type of data. In the above example, KNN with NONE 
% alignment is better than SVM with NONE alignment. But you find a dataset
% for which the opposite is true. In summary, the classification accuracy
% depends on three main factors:
%
% * classification method (KNN,SVM)
% * alignment method (NONE,DTW,CDTW)
% * data (Synthetic Control,...)
%% Tuning soft margin parameter of SVM
% Like many other machine learning algorithms, SVM has also some parameters
% that controls the details of the algorithm. The most well known parameter
% is the soft margin parameter; usually denoted by C. It is very useful for
% the cases where it is impossible to obtain a separating hyperlane without
% any misclassification. Setting C to a non-zero value, you basically give
% some flexibility to SVM. In doing so, SVM is allowed to make
% some errors. Increasing the soft margin parameter give higher
% flexibility. For further information, you can take a look at 
% Cortes, C.;Vapnik, V. (1995), "Support-vector networks". Machine Learning
% 20 (3): 273 <http://dx.doi.org/10.1007%2FBF00994018
% doi:10.1007/BF00994018>
%%
% The default soft margin parameter is 8 in TSCU. You can change it by
% using |SVMSoftMargin| option. Here I will try 11 different parameters and
% plot the resulting classification accuracies. It is common to try powers
% of 2, so I will follow the general trend.
% The parameters that I used are C={pow(2,i) where i=-5,...,5}
margins=2.^(-5:5);
accuracies=zeros(1,numel(margins));
for i=1:length(margins)
    out=tscu(trn,tst,'Classifier','SVM','SVMSoftMargin',margins(i),...
        'LogLevel','Alert');
    accuracies(i)=out.perf.OA;
end
%%
% Then I will run tscu in silent mode (remember the |LogLevel| option),
% collect the accuracies and plot them. 
figure
plot(log2(margins),100*accuracies,'o-');
xlabel('Soft margin parameter (log2)');
ylabel('Overall accuracy (%)');
%%%
% Here are the classificaton accuracies over different soft margins. As
% you see, the results didn't change except C=1/8 and C=1/4. But the change
% in the accuracy is around 0.006 which is very small and negligible.
% Therefore, changing the soft margin parameter of SVM didn't
% improve the accuracy at least in this dataset and alignment scheme. 
%% Using nonlinear kernel for SVM
% TSCU allows you to change the default linear kernel used in SVM to a
% nonlinear gaussian kernel. For this, you have assign |SVMKernel| option
% to |gaussian|. 
tscu(trn,tst,'Classifier','SVM','SVMKernel','gaussian');
%%
% You can see the kernel type of SVM in the info messages. Other than that,
% the rest of the messages are same as before except the classification
% accuracy: 50% which is very low compared to the linear SVM.
% The confusion matrix tells us that the first class (normal) is confused 
% with the other classes. 
%% Tuning gaussian kernel
% You can change the gaussian kernel parameter sigma by using the option
% |SVMSigma|. Here I will try the following sigma values:
% s={pow(2,i) where i=-5,...,5}. Then I will run tscu in silent mode 
% (remember the |LogLevel| option) collect the accuracies and plot them. 
sigmas=2.^(-5:5);
accuracies=zeros(1,numel(sigmas));
for i=1:length(sigmas)
    out=tscu(trn,tst,'Classifier','SVM','SVMKernel','gaussian',...
        'SVMSigma',sigmas(i),...
        'LogLevel','Alert');
    accuracies(i)=out.perf.OA;
end
%%
figure
plot(log2(sigmas),100*accuracies,'o-');
xlabel('sigma parameter of Gaussian kernel (log2)');
ylabel('Overall accuracy (%)');
%% 
% As you see, the classification accuracy heavily depends of the sigma 
% parameter. As it is being increased, the overall accuracy increases. The
% largest accuracy (the y-axis of the biggest peak) is 97.67\%.  This is
% very high compared to the linear SVM.
fprintf('Maximum accuracy: %8.2f\n',100*max(accuracies));

##### SOURCE END #####
--></body></html>
